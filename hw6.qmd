---
title: "Homework 6"
author: "[Kate Miller]{style='background-color: yellow;'}"
toc: true
title-block-banner: true
title-block-style: default
execute: 
  freeze: true
  cache: true
  #format:
  #html: # comment this line to get pdf
  pdf: 
    fig-width: 7
    fig-height: 7
---


::: {.callout-important style="font-size: 0.8em;"}

Please read the instructions carefully before submitting your assignment.

1. This assignment requires you to only upload a `PDF` file on Canvas
1. Don't collapse any code cells before submitting. 
1. Remember to make sure all your code output is rendered properly before uploading your submission.

⚠️ Please add your name to the author information in the frontmatter before submitting your assignment ⚠️
:::


In this assignment, we will perform various tasks involving principal component analysis (PCA), principal component regression, and dimensionality reduction.

We will need the following packages:


```{R, message=FALSE, warning=FALSE, results='hide'}
packages <- c(
  "tibble",
  "dplyr", 
  "readr", 
  "tidyr", 
  "purrr", 
  "broom",
  "magrittr",
  "corrplot",
  "car"
)
# renv::install(packages)
sapply(packages, require, character.only=T)
```

<br><br><br><br>
---

## Question 1
::: {.callout-tip}
## 70 points
Principal component anlaysis and variable selection
:::

###### 1.1 (5 points)


The `data` folder contains a `spending.csv` dataset which is an illustrative sample of monthly spending data for a group of $5000$ people across a variety of categories. The response variable, `income`, is their monthly income, and objective is to predict the `income` for a an individual based on their spending patterns.

Read the data file as a tibble in R. Preprocess the data such that:

1. the variables are of the right data type, e.g., categorical variables are encoded as factors
2. all column names to lower case for consistency
3. Any observations with missing values are dropped

```{R, echo = TRUE}
library(readr)
library(dplyr)
library(tidyr)
path <- "data/spending.csv"

df <- read_csv(path)

df <- df %>%
  mutate_all(as.numeric) %>%  # Makes all numeric since there are no categorial variables
  drop_na() # Assures no NAs in the data

df <- na.omit(df)                              

head(df)
```

---

###### 1.2 (5 points)

Visualize the correlation between the variables using the `corrplot()` function. What do you observe? What does this mean for the model?

```{R, echo = TRUE}
library(corrplot)
correlation_matrix <- cor(df)

corrplot(correlation_matrix, method = "circle") #Utilizes corrplot method
```
Throughout the model, there are only very dark blue circles, which means there is a strong positive correlation between several of the variables with one another. Additionally, there are about half of the variables appear to have a correlation with one another. However, there are numerous white spaces, where many variables have absolutely no correlation with one another. In terms of the model, this means there will be multicollinearity, which could pose some issues for the model. However, the model will likely be highly significant since there is so much correlation.

I could not figure out how to do the df %>% outline given, so I erased that and did my own line of code.
---

###### 1.3 (5 points)

Run a linear regression model to predict the `income` variable using the remaining predictors. Interpret the coefficients and summarize your results. 


```{R, echo = TRUE}
model <- lm(income ~ ., data = df) # Uses lm function
summary(model)
```

Three asterisks indicates the most significant coefficients, and several variables have these asterisks. Likewise, no asterisk means that the variable is not that significant. The estimate column for each variable shows the coefficients which represent the change in income for a one-unit increase in the predictor variable. So, for example, for accessories, with a coefficient of 0.299876, means that a one unit increase in spending on accessories is associated with an increase in income of about $0.30. These would be different based on the predictor, but that is a generalistic interpretation.

The R-Squared value of 0.999 indicates that the model explains almost all the variance in the income variable. The F-Statistic value of 2.2e-16 shows that the model is highly significant overall. In summary, the model is accurate and significant overall when analyzing the income variable across the predictor variables.
---

###### 1.3 (5 points)

Diagnose the model using the `vif()` function. What do you observe? What does this mean for the model?

```{R, echo = TRUE}
library(car)

vif_values <- vif(model)

vif_values
```
Almost all variables seen above have really high VIF values, which does pose a problem. Variables with high VIF values (above 10) indicate strong multicollinearity with other predictor variables in the model. High VIF numbers can change the standard deviation and regression coefficients in the model, making them less reliable.
---

###### 1.4 (5 points)

Perform PCA using the `princomp` function in R. Print the summary of the PCA object.

```{R, echo = TRUE}
pca_no_income <- df[, !names(df) == "income"] # Finds everything except income

pca <- princomp(pca_no_income, cor = TRUE) 

summary(pca)
```

---

###### 1.5 (5 points)

Make a screeplot of the proportion of variance explained by each principal component. How many principal components would you choose to keep? Why?

```{R, echo = TRUE}

prop_var <- pca$sdev^2 / sum(pca$sdev^2) # Calculates proportion of variance

library(ggplot2)
qplot(c(1:length(prop_var)), prop_var) +
  geom_line() +
  xlab("Principal Component") +
  ylab("Property of Variance") +
  ggtitle("Scree Plot") +
  ylim(0, 1)
```
Based on the plot, I would keep the first two principal components, since after the second component, there is a clear elbow, which means that additional components contribute less to the variance of the data, and therefore should not be kept. By gathering these two components, we can capture the majority of the variance and presevere the necessary information.

###### 1.6 (5 points)

By setting any factor loadings below $0.2$ to $0$, summarize the factor loadings for the principal components that you chose to keep. 

```{R, echo = TRUE}
factor_loadings <- pca$loadings # Finds factor loadings
factor_loadings[abs(factor_loadings) < 0.2] <- 0 # Filters factor loadings as given in assignment requirements

factor_loadings

```
The factor loadings represent the correlation between each variable and the principal component, and the higher the value, the higher the correlation. Since I chose to keep components 1 and 2, I will be discussing those. For component 1, the variables of accessories, audio equipment, cameras, and clothing have relatively high numbers, suggesting that they correlate strongly to this component. For componenet 2, the variables of books and clothing have high loadings, meaning they likely contribute to this component.

Visualize the factor loadings. 

```{R, echo = TRUE}
heatmap(factor_loadings, 
        Rowv = NA, Colv = NA, 
        col = cm.colors(256),
        scale = "none", 
        main = "Factor Loadings Heatmap")
```

---

###### 1.7 (15 points)

Based on the factor loadings, what do you think the principal components represent? 

Provide an interpretation for each principal component you chose to keep.

I chose to do a heat map to visualize the factor loadings. The pink in this case means positive correlation, and the blue means negative correlation. The darker the color, the more correlated two items are. Based on the factor loadings, I think the components likely represent how different variables are connected or just a general connection between them. 

For component 1, the principal components represent patterns related to personal items and technology, since the variables of accessories, clothing, laptops, smartphones, and tablets all have high loadings and correlation to component 1. For component 2, which has high loadings for magazines and movies, there are patterns related to media and entertainment consumption. As you can see with the heatmap, component 1 has numerous different correlations with different variables, meaning that there would be lots of different patterns to look for between the variables.
---

###### 1.8 (10 points)

Create a new data frame with the original response variable `income` and the principal components you chose to keep. Call this data frame `df_pca`.

```{R, echo = TRUE}
num_components <- 2 
pca_components <- pca$scores[, 1:num_components]

df_pca <- data.frame(income = df$income, pca_components) # Does pca with income

head(df_pca)
```

Fit a regression model to predict the `income` variable using the principal components you chose to keep. Interpret the coefficients and summarize your results. 

```{R, echo = TRUE}
pca_lm_model <- lm(income ~ ., data = df_pca)

summary(pca_lm_model)
```
For component 1, a one-unit increase in the first principal component results in an approximate 13 dollar increase in income. For component 2, a one-unit increase is associated with approximately a dollar decrease in income. The R-Squared value of 0.0277 means that not a lot of the variance in the income variable is explained by the principal components in the model. The p-value and the F-statistic show that the model is highly significant. 

In summary, this model has high significance, but a low R-Squared value, which could hinder its accuracy. 

Compare the results of the regression model in 1.3 and 1.9. What do you observe? What does this mean for the model?

```{R, echo = TRUE}
r_squared_original <- summary(model)$r.squared
r_squared_pca <- summary(pca_lm_model)$r.squared

coefficients_original <- coef(model)[-1] 
coefficients_pca <- coef(pca_lm_model)[-1]

r_squared_original
r_squared_pca
coefficients_original
coefficients_pca

```
I observe that the R-Squared value for the original model is a lot closer to 1 than for the pca model. I'm not sure if this was what was supposed to happen, but that is what my results are showing. This means that the original model would be a better fit for determining different conclusions accurately for the data. Originally, my R-Squared for the pca model was 0.994, so very close and very similar and very accurate, though when I ran the code chunk again, it was significantly lower. 

---

###### 1.10 (10 points)

Based on your interpretation of the principal components from Question 1.7, provide an interpretation of the regression model in Question 1.9.

Based on the principal components, the regression model in Question 1.9 does not appear to be a great fit. Though the components 1 and 2 were meaningful in terms of the data, my Question 1.9 does not support this and provides a different conclusion. Originally, my R-Squared for the pca model was 0.994, so very close and very similar and very accurate, though when I ran the code chunk again, it was significantly lower. 

---


:::{.hidden unless-format="pdf"}
\pagebreak
:::

<br><br><br><br>
<br><br><br><br>
---



::: {.callout-note collapse="true"}
## Session Information

Print your `R` session information using the following command

```{R}
sessionInfo()
```
:::